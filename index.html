<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Human Pose, Human Mesh, Virtual Human, 3D Reconstruction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Learning Dense Hand Contact Estimation from Imbalanced Data</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon_snu.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Learning Dense Hand Contact Estimation from Imbalanced Data</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://dqj5182.github.io/">Daniel Sungho Jung*</a>,
            </span>
            <span class="author-block">
              <a href="https://cv.snu.ac.kr/index.php/~kmlee/">Kyoung Mu Lee</a>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Seoul National University,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">ArXiv 2025</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2505.11152"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2505.11152"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/dqj5182/HACO_RELEASE"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/contho_overview.png" height="150%">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">CONTHO</span> reconstructs 3D human and object by exploiting human-object contact.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Human-object contact serves as a strong cue to understand how humans physically interact with objects. 
            Nevertheless, it is not widely explored to utilize human-object
            contact information for the joint reconstruction of 3D
            human and object from a single image. In this work,
            we present a novel joint 3D human-object reconstruction
            method (CONTHO) that effectively exploits contact information between humans and objects. 
            There are two core designs in our system: 1) 3D-guided contact estimation and 2)
            contact-based 3D human and object refinement. First, for
            accurate human-object contact estimation, CONTHO initially reconstructs 3D humans and objects and utilizes them
            as explicit 3D guidance for contact estimation. Second, to
            refine the initial reconstructions of 3D human and object,
            we propose a novel contact-based refinement Transformer
            that effectively aggregates human features and object features based on the estimated human-object contact. 
            The proposed contact-based refinement prevents the learning of
            erroneous correlation between human and object, which enables accurate 3D reconstruction. 
            As a result, our CONTHO achieves state-of-the-art performance in both human-object contact estimation and joint reconstruction of 3D human and object. 
            The code is publicly available.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Intro Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/ETwnMLyqHrk"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->

  </div>
</section>


  

<section class="section">
  <div class="container is-max-desktop">

    <!-- Proposed method -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Proposed Method</h2>
        <div class="content has-text-justified">
          <p>
            Our method first reconstructs 3D human and object meshes. 
            Then, the initial meshes are utilized to construct 3D vertex features. 
            Subsequently, ContactFormer estimates human-object contact maps from the 3D vertex features. 
            Lastly, CRFormer aggregates the 3D vertex features based on the estimated contact maps to provide refined human and object meshes. 
            The green color indicates the estimated contacting regions.
          </p>
        </div>
        <img src="./static/images/contho_pipeline.png" height="250%">
      </div>
    </div>
    <!--/ Proposed method -->

    <!-- Undesired HO -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Undesired Human-Object Correlation</h2>
        <div class="content has-text-justified">
          <p>
            We conduct a sensitivity test, inspecting which region is sensitive in reconstruction, 
            for Transformer baseline and our CRFormer. 
            In the Transformer baseline, the object errors are sensitive to human regions not actually related to human-object
            interaction, as a result of undesired correlation.
            In our CRFormer, the object errors are mostly sensitive around regions containing human-object contact.
          </p>
        </div>
        <img src="./static/images/undesired_ho_correlation.png" height="250%">
      </div>
    </div>
    <!--/ Undesired HO -->

    <!-- Results: 3D Recon -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitative Results on 3D Reconstruction</h2>
        <img src="./static/images/qualitative_recon.png" height="250%">
      </div>
    </div>
    <!--/ Results: 3D Recon -->

    <!-- Results: Contact -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitative Results on Contact Estimation</h2>
        <img src="./static/images/qualitative_contact.png" height="250%">
      </div>
    </div>
    <!--/ Results: Contact -->


    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent works that we wish to share.
          </p>
          <p>
            <a href="https://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Moon_Accurate_3D_Hand_Pose_Estimation_for_Whole-Body_3D_Human_Mesh_CVPRW_2022_paper.pdf">Hand4Whole: Accurate 3D Hand Pose Estimation for Whole-Body 3D Human Mesh Estimation.
          </p>
          <p>
            <a href="https://virtualhumans.mpi-inf.mpg.de/chore/chore.pdf">CHORE: Contact, Human and Object REconstruction from a single RGB image.</a>
          </p>
          <p>
            <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Tripathi_DECO_Dense_Estimation_of_3D_Human-Scene_Contact_In_The_Wild_ICCV_2023_paper.pdf">DECO: Dense Estimation of 3D Human-Scene Contact in the Wild.</a>
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{nam2024contho,    
title = {Joint Reconstruction of 3D Human and Object via Contact-Based Refinement Transformer},
author = {Nam, Hyeongjin and Jung, Daniel Sungho and Moon, Gyeongsik and Lee, Kyoung Mu},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},  
year = {2024}  
}  
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            The code for this webpage is largely borrowed from the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
